{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Rogue Wave Data with Random Forest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Imports\n",
    "\n",
    "Importing all required packages and define seed and number of cores to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.path.append('./')\n",
    "import utils\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 cores from 8 available cores.\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "n_jobs = 4\n",
    "print(f\"Using {n_jobs} cores from {os.cpu_count()} available cores.\") # how many CPU cores are available on the current machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = True\n",
    "num_cv = 10\n",
    "case = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an ElasticNet Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the Model and Setting Hyperparameters\n",
    "\n",
    "- `C`: Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "- `l1_ratio`: The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if penalty='elasticnet'. Setting l1_ratio=0 is equivalent to using penalty='l2', while setting l1_ratio=1 is equivalent to using penalty='l1'. For 0 < l1_ratio <1, the penalty is a combination of L1 and L2.\n",
    "- `class_weight`: Weights associated with classes. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grid = {'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 20.0, 30., 4.0, 5.0, 10.0, 100.0], \n",
    "            'l1_ratio': [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1],\n",
    "            'class_weight': ['balanced', None] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elnet_model(seed, X_train, X_test, y_train_cat, y_test_cat):\n",
    "    classifier = LogisticRegression(solver='saga', penalty = 'elasticnet', random_state=seed, n_jobs=n_jobs)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_transformed = scaler.fit_transform(X_train)\n",
    "    X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    skf = StratifiedKFold(n_splits=num_cv).split(X_train_transformed, y_train_cat)\n",
    "\n",
    "    gridsearch_classifier = GridSearchCV(classifier, hyperparameter_grid, cv=skf)\n",
    "    gridsearch_classifier.fit(X_train_transformed, y_train_cat)\n",
    "\n",
    "    # Check the results\n",
    "    print(f'The mean cross-validated score of the best model is {round(gridsearch_classifier.best_score_*100, 2)}% accuracy and the parameters of best prediction model are:')\n",
    "    print(gridsearch_classifier.best_params_)\n",
    "\n",
    "    # Take the best estimator\n",
    "    model = gridsearch_classifier.best_estimator_\n",
    "\n",
    "    # Predict training labels\n",
    "    y_pred = model.predict(X_train_transformed)\n",
    "    y_true = y_train_cat\n",
    "\n",
    "    print(f\"Balanced acc: {balanced_accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"Macro F1 score: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "    print(f\"Confusion matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
    "\n",
    "    # Predict test labels\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    y_true = y_test_cat\n",
    "\n",
    "    print(f\"Balanced acc: {balanced_accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"Macro F1 score: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "    print(f\"Confusion matrix:\\n{confusion_matrix(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model\n",
    "\n",
    "For hyperparameter tuning, we use a k-fold crossvalidation with a stratified splitter that ensures we have enough data from each class in the training and validation set.\n",
    "\n",
    "For evaluation use confusion matrix, macro F1 score and balanced accuracy to account for potential class imbalances.\n",
    "\n",
    "We load the case 2 data that was preprocessed in `data_preprocessing.ipynb`.  \n",
    "\n",
    "Case 2: \n",
    "- class 0: target < 1.5\n",
    "- class 1: target > 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for case 2 with random undersampled data.\n",
      "\n",
      "Training dataset target distribution:\n",
      "Counter({0: 14264, 1: 14264})\n",
      "\n",
      "Test dataset target distribution:\n",
      "Counter({0: 83728, 1: 3566})\n",
      "The mean cross-validated score of the best model is 70.29% accuracy and the parameters of best prediction model are:\n",
      "{'C': 0.1, 'class_weight': 'balanced', 'l1_ratio': 1}\n",
      "Balanced acc: 0.7024326977005048\n",
      "Macro F1 score: 0.7024266131169831\n",
      "Confusion matrix:\n",
      "[[ 9955  4309]\n",
      " [ 4180 10084]]\n",
      "Balanced acc: 0.7043801810933012\n",
      "Macro F1 score: 0.48931246728229605\n",
      "Confusion matrix:\n",
      "[[58526 25202]\n",
      " [ 1035  2531]]\n"
     ]
    }
   ],
   "source": [
    "undersample_method = \"random\"\n",
    "\n",
    "print(f'Building model for case {case}' + f'{f\" with {undersample_method} undersampled data\" if undersample else \"\"}.')\n",
    "data_train, data_test, y_train, y_train_cat, X_train, y_test, y_test_cat, X_test = utils.load_data(case, undersample, undersample_method)\n",
    "elnet_model(seed, X_train, X_test, y_train_cat, y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for case 2 with nearmiss undersampled data.\n",
      "\n",
      "Training dataset target distribution:\n",
      "Counter({0: 14264, 1: 14264})\n",
      "\n",
      "Test dataset target distribution:\n",
      "Counter({0: 83728, 1: 3566})\n",
      "The mean cross-validated score of the best model is 60.9% accuracy and the parameters of best prediction model are:\n",
      "{'C': 0.1, 'class_weight': None, 'l1_ratio': 0}\n",
      "Balanced acc: 0.6198822209758834\n",
      "Macro F1 score: 0.6198506448894372\n",
      "Confusion matrix:\n",
      "[[8972 5292]\n",
      " [5552 8712]]\n",
      "Balanced acc: 0.5512648507213862\n",
      "Macro F1 score: 0.36924542544745725\n",
      "Confusion matrix:\n",
      "[[40822 42906]\n",
      " [ 1373  2193]]\n"
     ]
    }
   ],
   "source": [
    "undersample_method = \"nearmiss\"\n",
    "\n",
    "print(f'Building model for case {case}' + f'{f\" with {undersample_method} undersampled data\" if undersample else \"\"}.')\n",
    "data_train, data_test, y_train, y_train_cat, X_train, y_test, y_test_cat, X_test = utils.load_data(case, undersample, undersample_method)\n",
    "elnet_model(seed, X_train, X_test, y_train_cat, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Random Forest Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the Model and Setting Hyperparameters\n",
    "\n",
    "- `n_estimators`: The number of trees in the forest.\n",
    "- `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "- `max_samples`: If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "- `criterion`: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain.\n",
    "- `max_features`: The number of features to consider when looking for the best split.\n",
    "- `class weight`:\n",
    "  - The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "  - The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grid = {'n_estimators': [1000], \n",
    "            'max_depth': [5, 10, 20, 30, 50], \n",
    "            'max_samples': [0.5, 0.8, 0.95],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt','log2'],\n",
    "            'class_weight': ['balanced', 'balanced_subsample'] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model(seed, X_train, X_test, y_train_cat, y_test_cat):\n",
    "    # Define the classifier. We set the oob_score = True, as OOB is a good approximation of the validation set score\n",
    "    classifier = RandomForestClassifier(oob_score=True, random_state=seed)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    skf = StratifiedKFold(n_splits=num_cv).split(X_train, y_train_cat)\n",
    "    \n",
    "    gridsearch_classifier = GridSearchCV(classifier, hyperparameter_grid, cv=skf, verbose=0, n_jobs=n_jobs)\n",
    "    gridsearch_classifier.fit(X_train, y_train_cat)\n",
    "\n",
    "    # Check the results\n",
    "    print(f'The mean cross-validated score of the best model is {round(gridsearch_classifier.best_score_*100, 2)}% accuracy and the parameters of best prediction model are:')\n",
    "    print(gridsearch_classifier.best_params_)\n",
    "\n",
    "    # Take the best estimator\n",
    "    model = gridsearch_classifier.best_estimator_\n",
    "\n",
    "    # Predict training labels\n",
    "    y_pred = model.predict(X_train)\n",
    "    y_true = y_train_cat\n",
    "\n",
    "    print(f\"Balanced acc: {balanced_accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"Macro F1 score: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "    print(f\"Confusion matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
    "\n",
    "    # Predict test labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_true = y_test_cat\n",
    "\n",
    "    print(f\"Balanced acc: {balanced_accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"Macro F1 score: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "    print(f\"Confusion matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
    "\n",
    "    data_and_model = [data_train, data_test, model]\n",
    "\n",
    "    with open(f'../models/class_model_randomforest_case{case}{f\"_{undersample_method}_undersampled\" if undersample else \"\"}.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_and_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model\n",
    "\n",
    "For hyperparameter tuning, we use a k-fold crossvalidation with a stratified splitter that ensures we have enough data from each class in the training and validation set.\n",
    "\n",
    "For evaluation use confusion matrix, macro F1 score and balanced accuracy to account for potential class imbalances.\n",
    "\n",
    "We load the case 2 data that was preprocessed in `data_preprocessing.ipynb`.  \n",
    "\n",
    "Case 2: \n",
    "- class 0: target < 1.5\n",
    "- class 1: target > 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for case 2 with random undersampled data.\n",
      "\n",
      "Training dataset target distribution:\n",
      "Counter({0: 14264, 1: 14264})\n",
      "\n",
      "Test dataset target distribution:\n",
      "Counter({0: 83728, 1: 3566})\n",
      "The mean cross-validated score of the best model is 99.62% accuracy and the parameters of best prediction model are:\n",
      "{'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 50, 'max_features': 'sqrt', 'max_samples': 0.95, 'n_estimators': 1000}\n",
      "Balanced acc: 1.0\n",
      "Macro F1 score: 1.0\n",
      "Confusion matrix:\n",
      "[[14264     0]\n",
      " [    0 14264]]\n",
      "Balanced acc: 0.9971070693994141\n",
      "Macro F1 score: 0.9711103113974497\n",
      "Confusion matrix:\n",
      "[[83314   414]\n",
      " [    3  3563]]\n"
     ]
    }
   ],
   "source": [
    "undersample_method = \"random\"\n",
    "\n",
    "print(f'Building model for case {case}' + f'{f\" with {undersample_method} undersampled data\" if undersample else \"\"}.')\n",
    "data_train, data_test, y_train, y_train_cat, X_train, y_test, y_test_cat, X_test = utils.load_data(case, undersample, undersample_method)\n",
    "rf_model(seed, X_train, X_test, y_train_cat, y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for case 2 with nearmiss undersampled data.\n",
      "\n",
      "Training dataset target distribution:\n",
      "Counter({0: 14264, 1: 14264})\n",
      "\n",
      "Test dataset target distribution:\n",
      "Counter({0: 83728, 1: 3566})\n",
      "The mean cross-validated score of the best model is 96.93% accuracy and the parameters of best prediction model are:\n",
      "{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'max_samples': 0.8, 'n_estimators': 1000}\n",
      "Balanced acc: 1.0\n",
      "Macro F1 score: 1.0\n",
      "Confusion matrix:\n",
      "[[14264     0]\n",
      " [    0 14264]]\n",
      "Balanced acc: 0.8111308354569383\n",
      "Macro F1 score: 0.4770770780264054\n",
      "Confusion matrix:\n",
      "[[52359 31369]\n",
      " [   11  3555]]\n"
     ]
    }
   ],
   "source": [
    "undersample_method = \"nearmiss\"\n",
    "\n",
    "print(f'Building model for case {case}' + f'{f\" with {undersample_method} undersampled data\" if undersample else \"\"}.')\n",
    "data_train, data_test, y_train, y_train_cat, X_train, y_test, y_test_cat, X_test = utils.load_data(case, undersample, undersample_method)\n",
    "rf_model(seed, X_train, X_test, y_train_cat, y_test_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rogue_wave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
