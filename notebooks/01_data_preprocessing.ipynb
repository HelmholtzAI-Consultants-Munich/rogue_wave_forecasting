{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.path.append('./')\n",
    "import utils\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Rogue Wave Data\n",
    "\n",
    "Note:\n",
    "\n",
    "- The data are stored in a csv file of the size 1048574 x 29.\n",
    "- Modified ina's input dataset, now the encoded values as well as original wave dir and obj weather data are in a same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rogue_waves = pd.read_csv(\"ML_Matrix_full_encoded_st.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset has {data_rogue_waves.shape[1] - 1} variables that describe {data_rogue_waves.shape[0]} waves.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rogue_waves.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Some of the target values are close to 3, which is not realistic, hence, they are excluded from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_target = 2.7\n",
    "data_rogue_waves = data_rogue_waves.loc[data_rogue_waves.AI_10min < thr_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select key features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['H_s', 'lambda_40', 'lambda_30', 'L_deep', 's', 'mu', 'kh', 'T_p', 'eps', 'nu', 'Q_p', 'swell', 'v_wind', 'T_air', 'Delta_T', 'p', 'Delta_p_1h','AI_10min']\n",
    "data_rogue_waves = data_rogue_waves.loc[:,selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rogue_waves.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_distributions(dataset=data_rogue_waves, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification approach, we need to turn the target values into classes.  \n",
    "We will have a look at three different target transformations:\n",
    "\n",
    "- case 1: class 0: target < 2.0 and class 1: target > 2.0 \n",
    "- case 2: class 0: target < 1.5 and class 1: target > 2.0\n",
    "- case 3: class 0: target < 1.5, class 1: 1.5 < target < 2.0 and class 2: target > 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold\n",
    "threshold = 2.0\n",
    "\n",
    "# Binarize target \n",
    "data_rogue_waves_case1 = data_rogue_waves.copy()\n",
    "data_rogue_waves_case1['target'] = data_rogue_waves_case1['AI_10min'].apply(lambda x: 0 if x < threshold else 1)\n",
    "data_rogue_waves_case1['target'] = data_rogue_waves_case1['target'].astype(int)\n",
    "\n",
    "print('Dataset target distribution:')\n",
    "print(Counter(data_rogue_waves_case1['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for three classes\n",
    "threshold_1 = 1.5\n",
    "threshold_2 = 2.0\n",
    "\n",
    "# Binarize target \n",
    "data_rogue_waves_case2 = data_rogue_waves.copy()\n",
    "data_rogue_waves_case2['target'] = data_rogue_waves_case2['AI_10min'].apply(lambda x: 0 if x < threshold_1 else (0.5 if x < threshold_2 else 1))\n",
    "data_rogue_waves_case2 = data_rogue_waves_case2[data_rogue_waves_case2['target'].isin([0, 1])]\n",
    "data_rogue_waves_case2['target'] = data_rogue_waves_case2['target'].astype(int)\n",
    "\n",
    "print('Dataset target distribution:')\n",
    "print(Counter(data_rogue_waves_case2['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for three classes\n",
    "threshold_1 = 1.5\n",
    "threshold_2 = 2.0\n",
    "\n",
    "# Binarize target \n",
    "data_rogue_waves_case3 = data_rogue_waves.copy()\n",
    "data_rogue_waves_case3['target'] = data_rogue_waves_case3['AI_10min'].apply(lambda x: 0 if x < threshold_1 else (1 if x < threshold_2 else 2))\n",
    "data_rogue_waves_case3['target'] = data_rogue_waves_case3['target'].astype(int)\n",
    "\n",
    "print('Dataset target distribution:')\n",
    "print(Counter(data_rogue_waves_case3['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target = \"target\"\n",
    "col_features = data_rogue_waves_case1.columns.drop(col_target).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup train and test data\n",
    "\n",
    "Split data into train and test dataset with stratified split, to ensure having enough instances of the minority class in both train and test set. Then save the dataset to pickl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the high class imbalance we will undersample the larger class using NearMiss Undersampler. Near-miss is an algorithm that can help in balancing an imbalanced dataset. It can be grouped under undersampling algorithms and is an efficient way to balance the data. The algorithm does this by looking at the class distribution and randomly eliminating samples from the larger class. When two points belonging to different classes are very close to each other in the distribution, this algorithm eliminates the datapoint of the larger class thereby trying to balance the distribution.\n",
    "\n",
    "For expl of version argument and n_neighbours see https://hersanyagci.medium.com/under-sampling-methods-for-imbalanced-data-clustercentroids-randomundersampler-nearmiss-eae0eadcc145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate([data_rogue_waves_case1, data_rogue_waves_case2, data_rogue_waves_case3]):\n",
    "\n",
    "    # We will first separate the target and feature columns.\n",
    "    X = data[col_features]\n",
    "    y = data[col_target]\n",
    "\n",
    "    # split into train and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, train_size=0.80, random_state=seed)\n",
    "\n",
    "    X_train.reset_index(inplace=True, drop=True)\n",
    "    X_test.reset_index(inplace=True, drop=True)\n",
    "    y_train.reset_index(inplace=True, drop=True)\n",
    "    y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    print('Training dataset target distribution:')\n",
    "    print(Counter(y_train))\n",
    "\n",
    "    print('Test dataset target distribution:')\n",
    "    print(Counter(y_test))\n",
    "\n",
    "    # Save the data \n",
    "    data = [X_train, X_test, y_train, y_test]\n",
    "\n",
    "    with open(f'../data/data_case{i+1}.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Undersample the larger class\n",
    "    nm = NearMiss(version=1, sampling_strategy='auto', n_neighbors=5) \n",
    "    X_train, y_train = nm.fit_resample(X_train, y_train)    \n",
    "\n",
    "    print('Resampled dataset shape:')\n",
    "    print(Counter(y_train))\n",
    "\n",
    "    # After undersampling, drop the continuous target variable from the dataset as we only use the binarized version for classification.\n",
    "    X_train = X_train.drop(columns=['AI_10min'])\n",
    "    X_test = X_test.drop(columns=['AI_10min'])\n",
    "\n",
    "    data = [X_train, X_test, y_train, y_test]\n",
    "\n",
    "    # Save the data \n",
    "    with open(f'../data/data_case{i+1}_undersampled.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_model",
   "language": "python",
   "name": "kernel_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
