{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.path.append('./')\n",
    "import utils\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Rogue Wave Data\n",
    "\n",
    "Note:\n",
    "\n",
    "- The data are stored in a csv file of the size 1048574 x 29.\n",
    "- Modified ina's input dataset, now the encoded values as well as original wave dir and obj weather data are in a same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rogue_waves = pd.read_csv(\"ML_Matrix_full_encoded_st.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset has {data_rogue_waves.shape[1] - 1} variables that describe {data_rogue_waves.shape[0]} waves.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rogue_waves.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Some of the target values are close to 3, which is not realistic, hence, they are excluded from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_target = 2.7\n",
    "data_rogue_waves = data_rogue_waves.loc[data_rogue_waves.AI_10min < thr_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select key features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['H_s', 'lambda_40', 'lambda_30', 'L_deep', 's', 'mu', 'kh', 'T_p', 'eps', 'nu', 'Q_p', 'swell', 'v_wind', 'T_air', 'Delta_T', 'p', 'Delta_p_1h','AI_10min']\n",
    "data_rogue_waves = data_rogue_waves.loc[:,selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rogue_waves.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_distributions(dataset=data_rogue_waves, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification approach, we need to turn the target values into classes.  \n",
    "We will have a look at three different target transformations:\n",
    "\n",
    "- case 1: class 0: target < 2.0 and class 1: target > 2.0 \n",
    "- case 2: class 0: target < 1.5 and class 1: target > 2.0\n",
    "- case 3: class 0: target < 1.5, class 1: 1.5 < target < 2.0 and class 2: target > 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold\n",
    "threshold = 2.0\n",
    "\n",
    "# Binarize target \n",
    "data_rogue_waves_case1 = data_rogue_waves.copy()\n",
    "data_rogue_waves_case1['target'] = data_rogue_waves_case1['AI_10min'].apply(lambda x: 0 if x < threshold else 1)\n",
    "data_rogue_waves_case1['target'] = data_rogue_waves_case1['target'].astype(int)\n",
    "\n",
    "print('Dataset target distribution:')\n",
    "print(Counter(data_rogue_waves_case1['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for three classes\n",
    "threshold_1 = 1.5\n",
    "threshold_2 = 2.0\n",
    "\n",
    "# Binarize target \n",
    "data_rogue_waves_case2 = data_rogue_waves.copy()\n",
    "data_rogue_waves_case2['target'] = data_rogue_waves_case2['AI_10min'].apply(lambda x: 0 if x < threshold_1 else (0.5 if x < threshold_2 else 1))\n",
    "data_rogue_waves_case2 = data_rogue_waves_case2[data_rogue_waves_case2['target'].isin([0, 1])]\n",
    "data_rogue_waves_case2['target'] = data_rogue_waves_case2['target'].astype(int)\n",
    "\n",
    "print('Dataset target distribution:')\n",
    "print(Counter(data_rogue_waves_case2['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for three classes\n",
    "threshold_1 = 1.5\n",
    "threshold_2 = 2.0\n",
    "\n",
    "# Binarize target \n",
    "data_rogue_waves_case3 = data_rogue_waves.copy()\n",
    "data_rogue_waves_case3['target'] = data_rogue_waves_case3['AI_10min'].apply(lambda x: 0 if x < threshold_1 else (1 if x < threshold_2 else 2))\n",
    "data_rogue_waves_case3['target'] = data_rogue_waves_case3['target'].astype(int)\n",
    "\n",
    "print('Dataset target distribution:')\n",
    "print(Counter(data_rogue_waves_case3['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target = \"target\"\n",
    "col_features = data_rogue_waves_case1.columns.drop(col_target).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup train and test data\n",
    "\n",
    "Split data into train and test dataset with stratified split, to ensure having enough instances of the minority class in both train and test set. Then save the dataset to pickl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate([data_rogue_waves_case1, data_rogue_waves_case2, data_rogue_waves_case3]):\n",
    "\n",
    "    # We will first separate the target and feature columns.\n",
    "    X = data[col_features]\n",
    "    y = data[col_target]\n",
    "\n",
    "    # split into train and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, train_size=0.80, random_state=seed)\n",
    "\n",
    "    X_train.reset_index(inplace=True, drop=True)\n",
    "    X_test.reset_index(inplace=True, drop=True)\n",
    "    y_train.reset_index(inplace=True, drop=True)\n",
    "    y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    print('Training dataset target distribution:')\n",
    "    print(Counter(y_train))\n",
    "\n",
    "    print('Test dataset target distribution:')\n",
    "    print(Counter(y_test))\n",
    "\n",
    "    # Save the model with joblib\n",
    "    data_and_model = [X_train, X_test, y_train, y_test]\n",
    "\n",
    "    with open(f'./data_case{i+1}.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_and_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
