{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c807e1",
   "metadata": {},
   "source": [
    "# Modelling Rogue Wave Data with Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a9805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a631bb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Imports\n",
    "\n",
    "Importing all required packages and define seed and number of cores to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35420465",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spearmanr\n\u001b[32m     15\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m./\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV, SequentialFeatureSelector\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "sys.path.append('./')\n",
    "sys.path.append('../scripts/')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eab7a6",
   "metadata": {},
   "source": [
    "### Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_jobs = 4\n",
    "print(f\"Using {n_jobs} cores from {os.cpu_count()} available cores.\") # how many CPU cores are available on the current machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1133292",
   "metadata": {},
   "source": [
    "We load the data that was preprocessed in `data_preprocessing.ipynb`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = \"../data/data_train_test.pickle\"  # path to the preprocessed data\n",
    "data_train, data_test, y_train, y_train_cat, X_train, y_test, y_test_cat, X_test = utils.load_data(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1bd75",
   "metadata": {},
   "source": [
    "## Building a Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e694ae4",
   "metadata": {},
   "source": [
    "### Instantiating the Model and Setting Hyperparameters\n",
    "\n",
    "- `n_estimators`: The number of trees in the forest.\n",
    "- `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "- `max_samples`: If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "- `criterion`: The function to measure the quality of a split. Supported criteria are “squared_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits. Training using “absolute_error” is significantly slower than when using “squared_error”.\n",
    "- `max_features`: The number of features to consider when looking for the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cv = 5\n",
    "\n",
    "hyperparameter_grid = {'n_estimators': [100], # not too high to reduce model size\n",
    "            'max_depth': [20, 30],\n",
    "            'max_samples': [0.3, 0.4, 0.5], # not too high to reduce model size and since we have many smaples we can afford to set it low\n",
    "            'criterion': ['friedman_mse'], # using friedman_mse because the target variable is approx. normally distributed but we can expect non-linear relationships or strong feature interactions since the ElNet model has very low performance\n",
    "            'max_features': ['sqrt'], # leave default since both sqrt and log2 lead to the same number of selected features, i.e. 4 since the total nu,mber of features is small (17)\n",
    "            'min_samples_leaf': [1, 2, 3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15f22b",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "For hyperparameter tuning, we use a k-fold crossvalidation with a stratified splitter that ensures we have enough rogue wave data in the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV\n",
    "model, cv_results = utils.run_CV(regressor, hyperparameter_grid, num_cv, X_train, y_train_cat, y_train)\n",
    "cv_results.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "file_cv = f\"../results/random_forest/cv_results.csv\"\n",
    "cv_results.to_csv(file_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.read_csv(file_cv)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f074803",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "The CV results above show that the top 3 performing models with crossvalidation score of R^2 > 0.9 are the models with 'max_depth' = 30 and 'max_samples' = 0.5 combined with 'min_samples_leaf' = 1 or 3, or 'max_samples' = 0.3 combined with 'min_samples_leaf' = 1.  \n",
    "We now check the model size of those top 3 models since the size has a large impact on the ability to run XAI analysis like SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a833b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_CV = [\n",
    "    {'max_samples': 0.5, 'min_samples_leaf': 1},\n",
    "    {'max_samples': 0.4, 'min_samples_leaf': 1},\n",
    "    {'max_samples': 0.5, 'min_samples_leaf': 2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cfda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyperparameters in hyperparameters_CV:\n",
    "    print(\"\\n\")\n",
    "    # Define the RF regressor\n",
    "    model = RandomForestRegressor(oob_score=False, random_state=seed, criterion='friedman_mse', max_depth=30, max_features='sqrt', n_estimators=100, max_samples=hyperparameters['max_samples'], min_samples_leaf=hyperparameters['min_samples_leaf'])\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    utils.evaluate_best_regressor(model, X_train, y_train, dataset=\"Training\", plot=False)\n",
    "\n",
    "    file_model_size_test = f'./model_size_test.pickle'\n",
    "    with open(file_model_size_test, 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Get the size of the saved file\n",
    "    model_size_gb = os.path.getsize(file_model_size_test) / (1024 ** 3)\n",
    "    print(f\"Model size on disk: {model_size_gb:.4f} GB\")\n",
    "\n",
    "    os.remove(file_model_size_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffaffe7",
   "metadata": {},
   "source": [
    "We choose to use the model with ```max_samples=0.5``` and ```min_samples_leaf=2``` as this seems to be a good tradeoff between model size and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(oob_score=False, random_state=seed, criterion='friedman_mse', max_depth=30, max_features='sqrt', n_estimators=100, max_samples=0.5, min_samples_leaf=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "utils.evaluate_best_regressor(model, X_train, y_train, dataset=\"Training\")\n",
    "utils.evaluate_best_regressor(model, X_test, y_test, dataset=\"Test\")\n",
    "\n",
    "# Save the model\n",
    "data_and_model = [data_train, data_test, model]\n",
    "\n",
    "file_data_model = f\"../results/random_forest/model_and_data.pickle\"\n",
    "with open(file_data_model, 'wb') as handle:\n",
    "    pickle.dump(data_and_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02698b2b",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "For the regression model we would like to reduce the number of required features, which would make it easier to use as forecasting model.  \n",
    "Some of the recoded features are rather hard to measure, e.g. *lambda_30*, *lamda_40*, *s*, *Delta_T*, *nu*, *Q_p*, while other are easier to measure, e.g. *wind*, *Hs*, *p*, *swell*, *kh*, *T_air*, *L_deep*, *T_p*, *Delta_p_1h*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7983e",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full, X_train, y_train, y_train_cat, X_test, y_test, y_test_cat = utils.load_data_and_model(file_data_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd6a8c",
   "metadata": {},
   "source": [
    "Check the correlation matrix to see if we have redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_correlation_matrix(X_train, figsize=(5, 5), annot=False, labelsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76423646",
   "metadata": {},
   "source": [
    "### Feature Selection with Recursive Feature Elimination\n",
    "\n",
    "A Recursive Feature Elimination (RFE) example with automatic tuning of the number of features selected with cross-validation.  \n",
    "For further information see:\n",
    "- https://scikit-learn.org/1.5/auto_examples/feature_selection/plot_rfe_with_cross_validation.html\n",
    "- https://medium.com/@loyfordmwenda/recursive-feature-rfe-elimination-with-scikit-learn-d0d29e96273d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98741ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regression model and use the hyperparameters from gridsearch for full regression model\n",
    "model_full, X_train, y_train, y_train_cat, X_test, y_test, y_test_cat = utils.load_data_and_model(file_data_model, output=False)\n",
    "params = model_full.get_params()\n",
    "\n",
    "# Initialize a new Random Forest Regressor with the same parameter settings\n",
    "model = RandomForestRegressor(**params)\n",
    "\n",
    "# Define splitter\n",
    "skf = StratifiedKFold(n_splits=num_cv).split(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RFE parameters\n",
    "min_features_to_select = 1  # Minimum number of features to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b000ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RFE\n",
    "rfecv = RFECV(\n",
    "    estimator=model,\n",
    "    step=1,\n",
    "    cv=skf,\n",
    "    scoring=\"r2\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=n_jobs,\n",
    ")\n",
    "rfecv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244780f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "cv_results = pd.DataFrame(rfecv.cv_results_)\n",
    "utils.plot_rfe(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(f\"Selected features: {X_train.columns[rfecv.support_]}\")\n",
    "print(f\"Unselected features: {X_train.columns[rfecv.support_ == False]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e76e2",
   "metadata": {},
   "source": [
    "The sequential feature selector returns 12 features leading to the best score. \n",
    "Hence, we will retrain the RF model with only those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = X_train.columns[rfecv.support_]\n",
    "\n",
    "print(f'Building model for top features: {top_features}.')\n",
    "\n",
    "model = RandomForestRegressor(**params)\n",
    "model.fit(X_train[top_features], y_train)\n",
    "\n",
    "# Train performance\n",
    "y_pred = model.predict(X_train[top_features])\n",
    "y_true = y_train\n",
    "\n",
    "print(f\"\\nTrain set MSE: {round(mean_squared_error(y_true, y_pred), 3)}\")\n",
    "print(f\"Train set R^2: {round(r2_score(y_true, y_pred), 3)}\")\n",
    "print(f\"Train set Spearman R: {round(spearmanr(y_true, y_pred).correlation, 3)}\")\n",
    "\n",
    "utils.plot_predictions(y_true=y_true, y_pred=y_pred, textstr = f'$R^2={round(r2_score(y_true, y_pred), 3)}$')\n",
    "\n",
    "# Test performance\n",
    "y_pred = model.predict(X_test[top_features])\n",
    "y_true = y_test\n",
    "\n",
    "print(f\"\\nTest set MSE: {round(mean_squared_error(y_true, y_pred), 3)}\")\n",
    "print(f\"Test set R^2: {round(r2_score(y_true, y_pred), 3)}\")\n",
    "print(f\"Test set Spearman R: {round(spearmanr(y_true, y_pred).correlation, 3)}\")\n",
    "\n",
    "utils.plot_predictions(y_true=y_true, y_pred=y_pred, textstr = f'$R^2={round(r2_score(y_true, y_pred), 3)}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfab806",
   "metadata": {},
   "source": [
    "### Sequential Feature Selection\n",
    "\n",
    "SFS is a greedy procedure where, at each iteration, we choose the best new feature to add to our selected features based a cross-validation score. That is, we start with 0 features and choose the best single feature with the highest score. The procedure is repeated until we reach the desired number of selected features.  \n",
    "For further information see:\n",
    "\n",
    "- https://scikit-learn.org/1.5/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ed91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regression model and use the hyperparameters from gridsearch for full regression model\n",
    "model_full, X_train, y_train, y_train_cat, X_test, y_test, y_test_cat = utils.load_data_and_model(file_data_model, output=False)\n",
    "params = model_full.get_params()\n",
    "\n",
    "# Initialize a new Random Forest Regressor with the same parameter settings\n",
    "model = RandomForestRegressor(**params)\n",
    "\n",
    "# Define splitter\n",
    "skf = StratifiedKFold(n_splits=num_cv).split(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SFS parameters\n",
    "tol = 0.05 # we use the r^2 score for a RandomForest Regressor\n",
    "n_features_to_select = \"auto\"\n",
    "direction = \"forward\"\n",
    "scoring = \"r2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c691a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SFS\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=n_features_to_select, tol=tol, direction=direction, scoring=scoring, cv=skf)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal number of features: {sfs.n_features_to_select_}\")\n",
    "print(f\"Selected features: {X_train.columns[sfs.support_]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80999d6",
   "metadata": {},
   "source": [
    "The sequential feature selector returns 4 features leading to the best score. \n",
    "Hence, we will retrain the RF model with only those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['H_s', 'lambda_40', 'T_p', 'v_wind']\n",
    "\n",
    "print(f'Building model for top features: {top_features}.')\n",
    "\n",
    "model = RandomForestRegressor(**params)\n",
    "model.fit(X_train[top_features], y_train)\n",
    "\n",
    "# Train performance\n",
    "y_pred = model.predict(X_train[top_features])\n",
    "y_true = y_train\n",
    "\n",
    "print(f\"\\nTrain set MSE: {round(mean_squared_error(y_true, y_pred), 3)}\")\n",
    "print(f\"Train set R^2: {round(r2_score(y_true, y_pred), 3)}\")\n",
    "print(f\"Train set Spearman R: {round(spearmanr(y_true, y_pred).correlation, 3)}\")\n",
    "\n",
    "utils.plot_predictions(y_true=y_true, y_pred=y_pred, textstr = f'$R^2={round(r2_score(y_true, y_pred), 3)}$')\n",
    "\n",
    "# Test performance\n",
    "y_pred = model.predict(X_test[top_features])\n",
    "y_true = y_test\n",
    "\n",
    "print(f\"\\nTest set MSE: {round(mean_squared_error(y_true, y_pred), 3)}\")\n",
    "print(f\"Test set R^2: {round(r2_score(y_true, y_pred), 3)}\")\n",
    "print(f\"Test set Spearman R: {round(spearmanr(y_true, y_pred).correlation, 3)}\")\n",
    "\n",
    "utils.plot_predictions(y_true=y_true, y_pred=y_pred, textstr = f'$R^2={round(r2_score(y_true, y_pred), 3)}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4381ca",
   "metadata": {},
   "source": [
    "## XAI for Random Forest Regression Model\n",
    "\n",
    "Next, we will use teh SHAP interpretability method for our Random Forest model to understand which features are important for Rogue Wave prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21794d0e",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765815d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, X_train, y_train, y_train_cat, X_test, y_test, y_test_cat = utils.load_data_and_model(file_data_model, output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce19bfe",
   "metadata": {},
   "source": [
    "### SHAP\n",
    "\n",
    "With SHAP we get contrastive explanations that compare the prediction with the average prediction. The global interpretations are consistent with the local explanations, since the Shapley values are the “atomic unit” of the global interpretations.\n",
    "\n",
    "When using TreeExplainer for a Ranfom Forest model, there will be small variations between the average model prediction and the expected value from SHAP. This behaviour is explained as follows in this GitHub thread:\n",
    "\n",
    "> It is because of how sklearn records the training samples in the tree models it builds. Random forests use a random subsample of the data to train each tree, and it is that random subsample that is used in sklearn to record the leaf sample weights in the model. Since TreeExplainer uses the recorded leaf sample weights to represent the training dataset, it will depend on the random sampling used during training. This will cause small variations like the ones you are seeing.\n",
    "\n",
    "To get the exact same values, we would have to sample from the whole background dataset for integrating out features via the Independent masker. However, since the dataset is so large, we have to use a smaller background dataset where we expect to see some deviations from the average model prediction.\n",
    "\n",
    "In addition, when using SHAP to explain a classifiers output, the default value in TreeExplainer for model_output=\"raw\", which explains the raw output of the model. For regression models, \"raw\" is the standard output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80470d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is executed on the server with: python xai_shap.py\n",
    "# import xai_shap\n",
    "# file_shap = xai_shap.run_shap(\n",
    "#     model_type=model_type, \n",
    "#     case=case, \n",
    "#     undersample_method=undersample_method, \n",
    "#     undersample=undersample, \n",
    "#     n_samples_0=46000, \n",
    "#     n_samples_1=90000, \n",
    "#     n_samples_2=14000, \n",
    "#     last_batch=-1, \n",
    "#     batch_size=1000, \n",
    "#     dir_output=f\"../results/random_forest/shap/\", \n",
    "#     n_jobs=n_jobs, \n",
    "#     seed=seed\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = f\"../results/random_forest/shap/dataset.pkl\"\n",
    "file_shap = f\"../results/random_forest/shap/shap.pkl\"\n",
    "\n",
    "# Load and unpack the data\n",
    "with open(file_data, \"rb\") as handle:\n",
    "    X_sample = pickle.load(handle)\n",
    "\n",
    "# Load and unpack the shap values\n",
    "with open(file_shap, \"rb\") as handle:\n",
    "    shap_values = pickle.load(handle)\n",
    "\n",
    "\n",
    "# Base value (model expected value)\n",
    "expected_value = model_full.predict(X_sample).mean()\n",
    "print(f'SHAP expected value is: {round(expected_value,2)}')\n",
    "\n",
    "# Create SHAP Explanation\n",
    "explanation = shap.Explanation(\n",
    "    values=shap_values,\n",
    "    base_values=np.full(len(X_sample), expected_value),\n",
    "    data=X_sample.values,\n",
    "    feature_names=X_sample.columns.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(explanation, max_display=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafaf305",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(explanation, max_display=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b7ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_shap_dependence(explanation, X_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
